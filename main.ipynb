{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d8fe641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "from os.path import abspath, dirname, join, basename, isdir\n",
    "import functools\n",
    "import json\n",
    "from addict import Dict\n",
    "import glob\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import torch.multiprocessing as mp\n",
    "import timm\n",
    "import shutil\n",
    "import time\n",
    "from dataset import TotalSegmentatorData\n",
    "from train import Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4764ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE = \"config/demo_config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96262081",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = Dict(yaml.load(open(CONFIG_FILE, \"r\"), Loader=yaml.Loader))\n",
    "paths = cfgs.paths\n",
    "data_cfgs = cfgs.dataset_params\n",
    "optim_cfgs = cfgs.optimizer_params\n",
    "schedule_cfgs = cfgs.scheduler_params\n",
    "model_cfgs = cfgs.model_params\n",
    "train_cfgs = cfgs.train_params\n",
    "test_cfgs = cfgs.test_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd55672",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e485792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveConfigFile(src, paths):\n",
    "    results_path = dirname(abspath(paths.model_ckpts_dest))\n",
    "    model_id = paths.model_ckpts_dest.split(\".\", 1)[0][-2:]\n",
    "    filename = basename(src).split(\".\", 1)[0] + \"_\" + model_id + \".yaml\"\n",
    "    cp_path = join(results_path, filename)\n",
    "\n",
    "    if not isdir(results_path):\n",
    "        os.makedirs(results_path)\n",
    "    shutil.copy(src, cp_path)\n",
    "    \n",
    "def TestDataLoader(cfile):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    cfgs = Dict(yaml.load(open(abspath(cfile), \"r\"), Loader=yaml.Loader))\n",
    "    train = Train(cfgs)\n",
    "    train.LoopDataset()\n",
    "\n",
    "def main(cfile):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    torch.manual_seed(42)\n",
    "    cfgs = Dict(yaml.load(open(abspath(cfile), \"r\"), Loader=yaml.Loader))\n",
    "    SaveConfigFile(CONFIG_FILE, cfgs.paths)\n",
    "    train = Train(cfgs)\n",
    "    train.RunDDP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95fdc105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgteam/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/sgteam/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/sgteam/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/sgteam/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/sgteam/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/isaiah/.local/lib/python3.9/site-packages/torchio/transforms/augmentation/spatial/random_elastic_deformation.py:271: RuntimeWarning: The maximum displacement is larger than the coarse grid spacing for dimensions: [2], so folding may occur. Choose fewer control points or a smaller maximum displacement\n",
      "  warnings.warn(message, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "main(CONFIG_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd8c8d3",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e2d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
