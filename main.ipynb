{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d8fe641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['CUDA_DEVICE_ORDER']='PCI_BUS_ID'\n",
    "# os.environ['CUDA_VISIBLE_DEVICES']='1'\n",
    "from os.path import abspath, dirname, join, basename, isdir\n",
    "import functools\n",
    "import json\n",
    "from addict import Dict\n",
    "import glob\n",
    "import yaml\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.functional as F\n",
    "from torch.utils.data import DataLoader, DistributedSampler\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "import torch.multiprocessing as mp\n",
    "import timm\n",
    "import shutil\n",
    "import time\n",
    "from dataset import TotalSegmentatorData\n",
    "from train import Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4764ff12",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_FILE = \"config/demo_config.yaml\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "96262081",
   "metadata": {},
   "outputs": [],
   "source": [
    "cfgs = Dict(yaml.load(open(CONFIG_FILE, \"r\"), Loader=yaml.Loader))\n",
    "paths = cfgs.paths\n",
    "data_cfgs = cfgs.dataset_params\n",
    "optim_cfgs = cfgs.optimizer_params\n",
    "schedule_cfgs = cfgs.scheduler_params\n",
    "model_cfgs = cfgs.model_params\n",
    "train_cfgs = cfgs.train_params\n",
    "test_cfgs = cfgs.test_params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd55672",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e485792",
   "metadata": {},
   "outputs": [],
   "source": [
    "def SaveConfigFile(src, paths):\n",
    "    results_path = dirname(abspath(paths.model_ckpts_dest))\n",
    "    model_id = paths.model_ckpts_dest.split(\".\", 1)[0][-2:]\n",
    "    filename = basename(src).split(\".\", 1)[0] + \"_\" + model_id + \".yaml\"\n",
    "    cp_path = join(results_path, filename)\n",
    "\n",
    "    if not isdir(results_path):\n",
    "        os.makedirs(results_path)\n",
    "    shutil.copy(src, cp_path)\n",
    "    \n",
    "def TestDataLoader(cfile):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    cfgs = Dict(yaml.load(open(abspath(cfile), \"r\"), Loader=yaml.Loader))\n",
    "    train = Train(cfgs)\n",
    "    train.LoopDataset()\n",
    "\n",
    "def main(cfile):\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    torch.manual_seed(42)\n",
    "    cfgs = Dict(yaml.load(open(abspath(cfile), \"r\"), Loader=yaml.Loader))\n",
    "    SaveConfigFile(CONFIG_FILE, cfgs.paths)\n",
    "    train = Train(cfgs)\n",
    "    train.RunDDP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95fdc105",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sgteam/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/sgteam/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/sgteam/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/sgteam/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/sgteam/miniconda3/lib/python3.9/site-packages/torch/optim/lr_scheduler.py:138: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n",
      "  warnings.warn(\"Detected call of `lr_scheduler.step()` before `optimizer.step()`. \"\n",
      "/home/isaiah/.local/lib/python3.9/site-packages/torchio/transforms/augmentation/spatial/random_elastic_deformation.py:271: RuntimeWarning: The maximum displacement is larger than the coarse grid spacing for dimensions: [2], so folding may occur. Choose fewer control points or a smaller maximum displacement\n",
      "  warnings.warn(message, RuntimeWarning)\n",
      "/home/isaiah/.local/lib/python3.9/site-packages/torchio/transforms/augmentation/spatial/random_elastic_deformation.py:271: RuntimeWarning: The maximum displacement is larger than the coarse grid spacing for dimensions: [2], so folding may occur. Choose fewer control points or a smaller maximum displacement\n",
      "  warnings.warn(message, RuntimeWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "0\n",
      "epoch 1/2 | block: 1, block_size: 1 | loss: 5.25586, dice: -0.003, PFbeta: 0.000, best: 0.000\n",
      "epoch 1/2 | block: 2, block_size: 1 | loss: 5.33143, dice: -0.003, PFbeta: 0.000, best: 0.000\n",
      "epoch 1/2 | block: 3, block_size: 1 | loss: 5.41859, dice: -0.002, PFbeta: 0.000, best: 0.000\n",
      "epoch 1/2 | block: 4, block_size: 1 | loss: 5.48373, dice: -0.002, PFbeta: 0.000, best: 0.000\n",
      "2\n",
      "1\n",
      "3\n"
     ]
    },
    {
     "ename": "ProcessRaisedException",
     "evalue": "\n\n-- Process 4 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/sgteam/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/isaiah/TotalSegmentatorProj/train.py\", line 213, in _TrainModelDDP\n    samples = torch.tensor(samples)\nValueError: too many dimensions 'str'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mProcessRaisedException\u001b[0m                    Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCONFIG_FILE\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 25\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(cfile)\u001b[0m\n\u001b[1;32m     23\u001b[0m SaveConfigFile(CONFIG_FILE, cfgs\u001b[38;5;241m.\u001b[39mpaths)\n\u001b[1;32m     24\u001b[0m train \u001b[38;5;241m=\u001b[39m Train(cfgs)\n\u001b[0;32m---> 25\u001b[0m \u001b[43mtrain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mRunDDP\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/TotalSegmentatorProj/utils/decorators.py:20\u001b[0m, in \u001b[0;36mTimeFuncDecorator.<locals>.TimeFunc.<locals>.Timer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m time_enabled:\n\u001b[1;32m     19\u001b[0m     start \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[0;32m---> 20\u001b[0m     \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m     end \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/TotalSegmentatorProj/train.py:269\u001b[0m, in \u001b[0;36mTrain.RunDDP\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;129m@TimeFuncDecorator\u001b[39m(\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    268\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mRunDDP\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 269\u001b[0m     \u001b[43mmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_TrainModelDDP\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mno_gpus\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/sgteam/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:240\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    236\u001b[0m     msg \u001b[38;5;241m=\u001b[39m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mThis method only supports start_method=spawn (got: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    237\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTo use a different start_method use:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    238\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m torch.multiprocessing.start_processes(...)\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m start_method)\n\u001b[1;32m    239\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(msg)\n\u001b[0;32m--> 240\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mstart_processes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdaemon\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mspawn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/home/sgteam/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:198\u001b[0m, in \u001b[0;36mstart_processes\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m context\n\u001b[1;32m    197\u001b[0m \u001b[38;5;66;03m# Loop on join until it returns True or raises an exception.\u001b[39;00m\n\u001b[0;32m--> 198\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[43mcontext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "File \u001b[0;32m/home/sgteam/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py:160\u001b[0m, in \u001b[0;36mProcessContext.join\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    158\u001b[0m msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m-- Process \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m terminated with the following error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m error_index\n\u001b[1;32m    159\u001b[0m msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m original_trace\n\u001b[0;32m--> 160\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ProcessRaisedException(msg, error_index, failed_process\u001b[38;5;241m.\u001b[39mpid)\n",
      "\u001b[0;31mProcessRaisedException\u001b[0m: \n\n-- Process 4 terminated with the following error:\nTraceback (most recent call last):\n  File \"/home/sgteam/miniconda3/lib/python3.9/site-packages/torch/multiprocessing/spawn.py\", line 69, in _wrap\n    fn(i, *args)\n  File \"/home/isaiah/TotalSegmentatorProj/train.py\", line 213, in _TrainModelDDP\n    samples = torch.tensor(samples)\nValueError: too many dimensions 'str'\n"
     ]
    }
   ],
   "source": [
    "main(CONFIG_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd8c8d3",
   "metadata": {},
   "source": [
    "## Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4e2d65",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
